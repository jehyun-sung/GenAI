{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1751851823018,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "OT8gKuhxddU8"
   },
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT_PATH = \"\"\n",
    "INPUT_PATH_1 = \"//.xlsx\"\n",
    "INPUT_PATH_2 = \"//.xlsx\"\n",
    "OUTPUT_PATH = \"//.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15763,
     "status": "ok",
     "timestamp": 1751851948819,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "IyJgz11BfQfi",
    "outputId": "05a810fb-1ed4-4a30-f3d8-75c913e47403"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486,
     "referenced_widgets": [
      "2f0dac876bfa4d4faac6cb0a8e8a351f",
      "82bc4e7d31d44dd4bba5e0cf3bb317dc",
      "6a8d3d180b4b4733b5d3f4b0a92064fe",
      "3ef3c323087c415cb0f5df370ff43085",
      "b16a60258e9545a8a1e5a2da39216346",
      "2c750598fbdc4a8a89f381c3f0d0863e",
      "a0aca995aa584bdbbf88f1ac272df504",
      "cf4fb780a01b47409a6acf0f1465906a",
      "53fe259db6d6453dad2aa49238d5cf5f",
      "fb8cdb32282940ee8cc3073756b1c91a",
      "521f20ef26d44c1da748dc554e112876"
     ]
    },
    "executionInfo": {
     "elapsed": 5707113,
     "status": "ok",
     "timestamp": 1751857661966,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "bcXuynrKmB32",
    "outputId": "1dd08eca-2643-4142-9045-56a3fa7f4f77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
      "WARNING:auto_gptq.modeling._base:Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
      "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
      "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
      "2. You are using pytorch without CUDA support.\n",
      "3. CUDA and nvcc are not installed in your device.\n",
      "WARNING:auto_gptq.modeling._base:CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
      "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
      "2. You are using pytorch without CUDA support.\n",
      "3. CUDA and nvcc are not installed in your device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading GPTQ model ‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO - The layer lm_head is not quantized.\n",
      "INFO:auto_gptq.modeling._base:The layer lm_head is not quantized.\n",
      "WARNING - MistralGPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.\n",
      "WARNING:auto_gptq.modeling._base:MistralGPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model ready.\n",
      "üìÑ Loading context ‚Ä¶\n",
      "‚úÖ Context rows: 24,579\n",
      "üìÑ Loading test csv ‚Ä¶\n",
      "‚úÖ Test rows   : 22,248\n",
      "üöÄ Total sentences to process: 22,248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0dac876bfa4d4faac6cb0a8e8a351f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üß† Processing:   0%|          | 0/696 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÅ Completed in 5692.8s  (22,248 rows)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 0. Installation (Colab A100) ‚Äì Skip if already installed\n",
    "# !pip install -q auto-gptq>=0.8.0 optimum \"flash-attn>=2.4.2.post1wjs\"\n",
    "# ==============================================================\n",
    "\n",
    "# ------------------ 1. Modules & Environment Setup ----------------------\n",
    "import re, yaml, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "\n",
    "INPUT_CSV      = f\"{INPUT_PATH_1}\"\n",
    "CONTEXT_XLSX   = f\"{INPUT_PATH_2}\"\n",
    "CONTEXT_SHEET  = \"\"\n",
    "BATCH_SIZE     = 32\n",
    "OUTPUT_NAME    = f\"{OUTPUT_PATH}\"\n",
    "BRAND_YML = Path(\".yaml\")\n",
    "ASPT_YML  = Path(\".yaml\")\n",
    "\n",
    "MODEL_ID      = \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\"  # 4-bit quantized model\n",
    "MAX_INPUT     = 8192  # Llama 3 8B context window\n",
    "MAX_NEW       = 32\n",
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ------------------ 2. Load GPTQ Model ------------------------\n",
    "print(\"üîß Loading GPTQ model ‚Ä¶\")\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    use_safetensors=True,\n",
    "    trust_remote_code=True,\n",
    "    inject_fused_attention=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = torch.compile(model)\n",
    "model.eval()\n",
    "print(\"‚úÖ Model ready.\")\n",
    "\n",
    "# ------------------ 3. Load Data ---------------------------\n",
    "print(\"üìÑ Loading context ‚Ä¶\")\n",
    "ctx_df = pd.read_excel(CONTEXT_XLSX)\n",
    "conversation_list = ctx_df[\"Conversation Stream\"].tolist()\n",
    "sent2idx = {s: i for i, s in enumerate(conversation_list)}\n",
    "print(f\"‚úÖ Context rows: {len(conversation_list):,}\")\n",
    "\n",
    "print(\"üìÑ Loading test csv ‚Ä¶\")\n",
    "test_df = pd.read_excel(INPUT_CSV)\n",
    "print(f\"‚úÖ Test rows   : {len(test_df):,}\")\n",
    "\n",
    "# =============================================================\n",
    "# 4. YAML ‚Üí Regex ‚Üí BrandClassifier  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def _load_yaml(path: Path) -> Dict[str, str]:\n",
    "    with path.open(encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def _compile_dict(src: Dict[str, str], flags: int = 0) -> Dict[str, re.Pattern]:\n",
    "    return {k: re.compile(v, flags) for k, v in src.items()}\n",
    "\n",
    "class BrandClassifier:\n",
    "    \"\"\"Regex-based classifier with LLM fallback\"\"\"\n",
    "\n",
    "    APPLE_REQ_ASP = {\n",
    "        'Camera General', 'Design General', 'Chipset General',\n",
    "        'Sustainability General', 'Portrait Studio',\n",
    "        'On-device', 'Grip', \"Thin\", \"AI General\"\n",
    "    }\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        brand_src  = _load_yaml(BRAND_YML)\n",
    "        aspect_src = _load_yaml(ASPT_YML)\n",
    "        self.brand_rx  = _compile_dict(brand_src)\n",
    "        self.aspect_rx = _compile_dict(aspect_src)\n",
    "        self._apple_kw_rx = re.compile(\n",
    "            r\"(?i)\\b(?:apple|apple's|iphone|ÏïÑÏù¥Ìè∞|Ïï†Ìîå)\\b\"\n",
    "        )\n",
    "\n",
    "    def classify(self, text: str, used_llm: bool = False) -> Tuple[str, str]:\n",
    "        # 1) Try brand first\n",
    "        for brand, brx in self.brand_rx.items():\n",
    "            if brx.search(text):\n",
    "                if brand == \"Apple\":\n",
    "                    for asp, arx in self.aspect_rx.items():\n",
    "                        if arx.search(text):\n",
    "                            return \"Apple\", asp\n",
    "                return brand, brand\n",
    "\n",
    "        # 2) No brand ‚Üí check aspect\n",
    "        for asp, arx in self.aspect_rx.items():\n",
    "            if arx.search(text):\n",
    "                if asp not in self.APPLE_REQ_ASP:\n",
    "                    return \"Apple\", asp\n",
    "                if self._apple_kw_rx.search(text):\n",
    "                    return \"Apple\", asp\n",
    "                return \"Unknown\", \"brand_required_aspect_detected\"\n",
    "\n",
    "        # 3) Nothing matched\n",
    "        return \"Unknown\", \"general\"\n",
    "\n",
    "clf = BrandClassifier()\n",
    "\n",
    "# ------------------ 6. LLM Prompt Template -------------------------\n",
    "SYS = \"You are an intelligent assistant.\"\n",
    "def _prompt(ctx: str, sent: str) -> str:\n",
    "    return (\n",
    "        f\"{SYS}\\nGiven a context, identify what the target sentence is describing. \"\n",
    "        \"Answer with one noun phrase only.\\n\"\n",
    "        \"\"\"        [Example 1]\n",
    "        <Context>\n",
    "        Flagship phones with great cameras have been IP-rated for a few years now.\n",
    "        I have had an Apple iPhone 15 Pro with me for a while now, but I never truly checked how effective the IP rating is except for a few splashes and washes here and there.\n",
    "        It gave me more confidence to take it along for walks even if it drizzled or light rains and have done so without much thought.\n",
    "        <Sentence>\n",
    "        It gave me more confidence to take it along for walks even if it drizzled or light rains and have done so without much thought.\n",
    "        <Answer>\n",
    "        IP rating of iPhone 15 Pro\n",
    "\n",
    "        [Example 2]\n",
    "        <Context>\n",
    "        Nova Earbuds come with noise cancellation and customizable touch controls.\n",
    "        - Intuitive Touch Control: Simple tap to play or pause music.\n",
    "        <Sentence>\n",
    "        - Intuitive Touch Control: Simple tap to play or pause music.\n",
    "        <Answer>\n",
    "        Nova Earbuds\"\"\"\n",
    "        f\"<Context>\\n{ctx}\\n<Sentence>\\n{sent}\\n<Question>What is the subject of this sentence given a context?\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "# ------------------ 7. LLM Inference -----------------------------\n",
    "def llm_batch(sent_list: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    prompts, ctx_list = [], []\n",
    "    for s in sent_list:\n",
    "        i   = sent2idx.get(s, -1)\n",
    "        ctx = \" \".join(conversation_list[max(0, i-3):i]) if i >= 0 else \"\"\n",
    "        ctx_list.append(ctx)\n",
    "        prompts.append(_prompt(ctx, s))\n",
    "\n",
    "    toks = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **toks,\n",
    "            max_new_tokens=MAX_NEW,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    dec = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "    return [d.split(\"Answer:\")[-1].strip() for d in dec], ctx_list\n",
    "\n",
    "# ------------------ 9. Batch Classification Logic ------------------------\n",
    "def classify_batch(texts: List[str]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Apply regex first ‚Üí fallback to LLM if unknown\"\"\"\n",
    "    results: List[Tuple[str, str]] = []\n",
    "    llm_raw: List[str]             = [\"\"] * len(texts)\n",
    "    llm_ctx: List[str]             = [\"\"] * len(texts)\n",
    "\n",
    "    pending_texts, idx_map = [], []\n",
    "\n",
    "    # 1) Regex-based classification\n",
    "    for i, t in enumerate(texts):\n",
    "        lab = clf.classify(t)\n",
    "        if lab[0] == \"Unknown\":\n",
    "            pending_texts.append(t); idx_map.append(i)\n",
    "            results.append(lab)\n",
    "        else:\n",
    "            results.append(lab)\n",
    "\n",
    "    # 2) Call LLM only for unknowns\n",
    "    if pending_texts:\n",
    "        llm_ans, ctx_list = llm_batch(pending_texts)\n",
    "        for ii, ans, ctx in zip(idx_map, llm_ans, ctx_list):\n",
    "            llm_raw[ii] = ans\n",
    "            llm_ctx[ii] = ctx\n",
    "            parsed = clf.classify(f\"{texts[ii]} (about {ans})\", used_llm=True)\n",
    "            results[ii] = parsed if parsed[0] != \"Unknown\" else (\"Unknown\", f\"about {ans}\")\n",
    "\n",
    "    return results, llm_raw, llm_ctx\n",
    "\n",
    "# ------------------ 10. Run Classification ----------------------------\n",
    "from tqdm.notebook import tqdm  # Make sure this is imported at the top\n",
    "\n",
    "def run(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    sents = df[\"Conversation Stream\"].tolist()\n",
    "    preds, llm_texts, ctx_list = [], [], []\n",
    "\n",
    "    print(f\"üöÄ Total sentences to process: {len(sents):,}\")\n",
    "    for i in tqdm(range(0, len(sents), BATCH_SIZE), desc=\"üß† Processing\", unit=\"batch\"):\n",
    "        p_batch, a_batch, c_batch = classify_batch(sents[i : i + BATCH_SIZE])\n",
    "        preds.extend(p_batch)\n",
    "        llm_texts.extend(a_batch)\n",
    "        ctx_list.extend(c_batch)\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"Predicted_Brand\"], out[\"Predicted_Aspect\"] = zip(*preds)\n",
    "    out[\"LLM Raw Answer\"] = llm_texts\n",
    "    out[\"LLM Context\"]    = ctx_list\n",
    "    return out\n",
    "\n",
    "# ------------------ 11. Save Results --------------------------------\n",
    "from zoneinfo import ZoneInfo  # Python 3.9+\n",
    "t0 = time.time()\n",
    "result_df = run(test_df)\n",
    "print(f\"üèÅ Completed in {time.time()-t0:.1f}s  ({len(result_df):,} rows)\")\n",
    "\n",
    "out_path = f\"{OUTPUT_NAME}\"\n",
    "result_df.to_excel(out_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNIHBnETDZyBurSQdVMW6m0",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2c750598fbdc4a8a89f381c3f0d0863e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f0dac876bfa4d4faac6cb0a8e8a351f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_82bc4e7d31d44dd4bba5e0cf3bb317dc",
       "IPY_MODEL_6a8d3d180b4b4733b5d3f4b0a92064fe",
       "IPY_MODEL_3ef3c323087c415cb0f5df370ff43085"
      ],
      "layout": "IPY_MODEL_b16a60258e9545a8a1e5a2da39216346"
     }
    },
    "3ef3c323087c415cb0f5df370ff43085": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb8cdb32282940ee8cc3073756b1c91a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_521f20ef26d44c1da748dc554e112876",
      "value": "‚Äá696/696‚Äá[1:34:52&lt;00:00,‚Äá‚Äá6.94s/batch]"
     }
    },
    "521f20ef26d44c1da748dc554e112876": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "53fe259db6d6453dad2aa49238d5cf5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a8d3d180b4b4733b5d3f4b0a92064fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf4fb780a01b47409a6acf0f1465906a",
      "max": 696,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_53fe259db6d6453dad2aa49238d5cf5f",
      "value": 696
     }
    },
    "82bc4e7d31d44dd4bba5e0cf3bb317dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c750598fbdc4a8a89f381c3f0d0863e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a0aca995aa584bdbbf88f1ac272df504",
      "value": "üß†‚ÄáProcessing:‚Äá100%"
     }
    },
    "a0aca995aa584bdbbf88f1ac272df504": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b16a60258e9545a8a1e5a2da39216346": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf4fb780a01b47409a6acf0f1465906a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb8cdb32282940ee8cc3073756b1c91a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
