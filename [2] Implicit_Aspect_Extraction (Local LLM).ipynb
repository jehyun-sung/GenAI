{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107465,
     "status": "ok",
     "timestamp": 1751851643333,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "SmvoawNBnYN5",
    "outputId": "a3c958f2-b156-4c85-c92d-ee7ff7f689e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/7.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/7.9 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m5.9/7.9 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q \"flash-attn>=2.4.2.post1\"  # CUDA 12용 / Colab A100\n",
    "# !pip install -q transformers accelerate bitsandbytes\n",
    "!pip install -q auto-gptq>=0.8.0 optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4222,
     "status": "ok",
     "timestamp": 1751851651816,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "WgR1kCJMcky2",
    "outputId": "da54ee6a-ad7b-4399-97a9-31593fc930a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: flash_attn 2.8.0.post2\n",
      "Uninstalling flash_attn-2.8.0.post2:\n",
      "  Successfully uninstalled flash_attn-2.8.0.post2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y flash-attn\n",
    "!rm -rf /usr/local/lib/python3.11/dist-packages/flash_attn*\n",
    "!rm -rf /root/.cache/pip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12932,
     "status": "ok",
     "timestamp": 1751851666408,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "gtqA0ULxija0",
    "outputId": "afb8174b-5f87-4f85-eac9-903668b7a20c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.53.0\n",
      "Uninstalling transformers-4.53.0:\n",
      "  Successfully uninstalled transformers-4.53.0\n",
      "Collecting transformers==4.41.0\n",
      "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.0)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (2025.6.15)\n",
      "Downloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.41.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y transformers\n",
    "!pip install transformers==4.41.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4603,
     "status": "ok",
     "timestamp": 1751851673185,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "c0cWj-kQaBzk",
    "outputId": "a6b145c8-efe7-4e80-c513-93f4f00d60d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.41.0 in /usr/local/lib/python3.11/dist-packages (4.41.0)\n",
      "Requirement already satisfied: peft==0.15.2 in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.15.2) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.15.2) (2.6.0+cu124)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.15.2) (1.8.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (1.1.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.15.2) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.15.2) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.15.2) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers==4.41.0 peft==0.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1751851823018,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "OT8gKuhxddU8"
   },
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT_PATH = \"/content/drive/MyDrive/9. Lab/BARQA/Packaging & Download/atsc/googleflan-t5-large-SA_E2E_250616_ver1/checkpoint-2496\"\n",
    "INPUT_PATH_1 = \"/content/implicit_case.xlsx\"\n",
    "INPUT_PATH_2 = \"/content/translated_crawl_results.xlsx\"\n",
    "OUTPUT_PATH = \"/content/Aspect_Extraction_implicit.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qG3LLbHne5Cv"
   },
   "outputs": [],
   "source": [
    "# %cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15763,
     "status": "ok",
     "timestamp": 1751851948819,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "IyJgz11BfQfi",
    "outputId": "05a810fb-1ed4-4a30-f3d8-75c913e47403"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12266,
     "status": "ok",
     "timestamp": 1751851837961,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "ZX6TFRgjRMhH",
    "outputId": "41d941ac-e7b7-452d-bd54-16f1d1b1e2f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.41.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Downloading transformers-4.53.1-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.41.0\n",
      "    Uninstalling transformers-4.41.0:\n",
      "      Successfully uninstalled transformers-4.41.0\n",
      "Successfully installed tokenizers-0.21.2 transformers-4.53.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486,
     "referenced_widgets": [
      "2f0dac876bfa4d4faac6cb0a8e8a351f",
      "82bc4e7d31d44dd4bba5e0cf3bb317dc",
      "6a8d3d180b4b4733b5d3f4b0a92064fe",
      "3ef3c323087c415cb0f5df370ff43085",
      "b16a60258e9545a8a1e5a2da39216346",
      "2c750598fbdc4a8a89f381c3f0d0863e",
      "a0aca995aa584bdbbf88f1ac272df504",
      "cf4fb780a01b47409a6acf0f1465906a",
      "53fe259db6d6453dad2aa49238d5cf5f",
      "fb8cdb32282940ee8cc3073756b1c91a",
      "521f20ef26d44c1da748dc554e112876"
     ]
    },
    "executionInfo": {
     "elapsed": 5707113,
     "status": "ok",
     "timestamp": 1751857661966,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "bcXuynrKmB32",
    "outputId": "1dd08eca-2643-4142-9045-56a3fa7f4f77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
      "WARNING:auto_gptq.modeling._base:Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
      "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
      "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
      "2. You are using pytorch without CUDA support.\n",
      "3. CUDA and nvcc are not installed in your device.\n",
      "WARNING:auto_gptq.modeling._base:CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
      "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
      "2. You are using pytorch without CUDA support.\n",
      "3. CUDA and nvcc are not installed in your device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Loading GPTQ model …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO - The layer lm_head is not quantized.\n",
      "INFO:auto_gptq.modeling._base:The layer lm_head is not quantized.\n",
      "WARNING - MistralGPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.\n",
      "WARNING:auto_gptq.modeling._base:MistralGPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model ready.\n",
      "📄 Loading context …\n",
      "✅ Context rows: 24,579\n",
      "📄 Loading test csv …\n",
      "✅ Test rows   : 22,248\n",
      "🚀 Total sentences to process: 22,248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0dac876bfa4d4faac6cb0a8e8a351f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "🧠 Processing:   0%|          | 0/696 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏁 Completed in 5692.8s  (22,248 rows)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 0. Installation (Colab A100) – Skip if already installed\n",
    "# !pip install -q auto-gptq>=0.8.0 optimum \"flash-attn>=2.4.2.post1wjs\"\n",
    "# ==============================================================\n",
    "\n",
    "# ------------------ 1. Modules & Environment Setup ----------------------\n",
    "import re, yaml, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "\n",
    "INPUT_CSV      = f\"{INPUT_PATH_1}\"\n",
    "CONTEXT_XLSX   = f\"{INPUT_PATH_2}\"\n",
    "CONTEXT_SHEET  = \"\"\n",
    "BATCH_SIZE     = 32\n",
    "OUTPUT_NAME    = f\"{OUTPUT_PATH}\"\n",
    "BRAND_YML = Path(\"/content/drive/MyDrive/9. Lab/BARQA/Packaging & Download/Data/brands_0519.yaml\")\n",
    "ASPT_YML  = Path(\"/content/drive/MyDrive/9. Lab/BARQA/Packaging & Download/Data/aspects_0519.yaml\")\n",
    "\n",
    "MODEL_ID      = \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\"  # 4-bit quantized model\n",
    "MAX_INPUT     = 8192  # Llama 3 8B context window\n",
    "MAX_NEW       = 32\n",
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ------------------ 2. Load GPTQ Model ------------------------\n",
    "print(\"🔧 Loading GPTQ model …\")\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    use_safetensors=True,\n",
    "    trust_remote_code=True,\n",
    "    inject_fused_attention=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = torch.compile(model)\n",
    "model.eval()\n",
    "print(\"✅ Model ready.\")\n",
    "\n",
    "# ------------------ 3. Load Data ---------------------------\n",
    "print(\"📄 Loading context …\")\n",
    "ctx_df = pd.read_excel(CONTEXT_XLSX)\n",
    "conversation_list = ctx_df[\"Conversation Stream\"].tolist()\n",
    "sent2idx = {s: i for i, s in enumerate(conversation_list)}\n",
    "print(f\"✅ Context rows: {len(conversation_list):,}\")\n",
    "\n",
    "print(\"📄 Loading test csv …\")\n",
    "test_df = pd.read_excel(INPUT_CSV)\n",
    "print(f\"✅ Test rows   : {len(test_df):,}\")\n",
    "\n",
    "# =============================================================\n",
    "# 4. YAML → Regex → BrandClassifier  ────────────────────────\n",
    "def _load_yaml(path: Path) -> Dict[str, str]:\n",
    "    with path.open(encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def _compile_dict(src: Dict[str, str], flags: int = 0) -> Dict[str, re.Pattern]:\n",
    "    return {k: re.compile(v, flags) for k, v in src.items()}\n",
    "\n",
    "class BrandClassifier:\n",
    "    \"\"\"Regex-based classifier with LLM fallback\"\"\"\n",
    "\n",
    "    APPLE_REQ_ASP = {\n",
    "        'Camera General', 'Design General', 'Chipset General',\n",
    "        'Sustainability General', 'Portrait Studio',\n",
    "        'On-device', 'Grip', \"Thin\", \"AI General\"\n",
    "    }\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        brand_src  = _load_yaml(BRAND_YML)\n",
    "        aspect_src = _load_yaml(ASPT_YML)\n",
    "        self.brand_rx  = _compile_dict(brand_src)\n",
    "        self.aspect_rx = _compile_dict(aspect_src)\n",
    "        self._apple_kw_rx = re.compile(\n",
    "            r\"(?i)\\b(?:apple|apple's|iphone|아이폰|애플)\\b\"\n",
    "        )\n",
    "\n",
    "    def classify(self, text: str, used_llm: bool = False) -> Tuple[str, str]:\n",
    "        # 1) Try brand first\n",
    "        for brand, brx in self.brand_rx.items():\n",
    "            if brx.search(text):\n",
    "                if brand == \"Apple\":\n",
    "                    for asp, arx in self.aspect_rx.items():\n",
    "                        if arx.search(text):\n",
    "                            return \"Apple\", asp\n",
    "                return brand, brand\n",
    "\n",
    "        # 2) No brand → check aspect\n",
    "        for asp, arx in self.aspect_rx.items():\n",
    "            if arx.search(text):\n",
    "                if asp not in self.APPLE_REQ_ASP:\n",
    "                    return \"Apple\", asp\n",
    "                if self._apple_kw_rx.search(text):\n",
    "                    return \"Apple\", asp\n",
    "                return \"Unknown\", \"brand_required_aspect_detected\"\n",
    "\n",
    "        # 3) Nothing matched\n",
    "        return \"Unknown\", \"general\"\n",
    "\n",
    "clf = BrandClassifier()\n",
    "\n",
    "# ------------------ 6. LLM Prompt Template -------------------------\n",
    "SYS = \"You are an intelligent assistant.\"\n",
    "def _prompt(ctx: str, sent: str) -> str:\n",
    "    return (\n",
    "        f\"{SYS}\\nGiven a context, identify what the target sentence is describing. \"\n",
    "        \"Answer with one noun phrase only.\\n\"\n",
    "        \"\"\"        [Example 1]\n",
    "        <Context>\n",
    "        Flagship phones with great cameras have been IP-rated for a few years now.\n",
    "        I have had an Apple iPhone 15 Pro with me for a while now, but I never truly checked how effective the IP rating is except for a few splashes and washes here and there.\n",
    "        It gave me more confidence to take it along for walks even if it drizzled or light rains and have done so without much thought.\n",
    "        <Sentence>\n",
    "        It gave me more confidence to take it along for walks even if it drizzled or light rains and have done so without much thought.\n",
    "        <Answer>\n",
    "        IP rating of iPhone 15 Pro\n",
    "\n",
    "        [Example 2]\n",
    "        <Context>\n",
    "        Nova Earbuds come with noise cancellation and customizable touch controls.\n",
    "        - Intuitive Touch Control: Simple tap to play or pause music.\n",
    "        <Sentence>\n",
    "        - Intuitive Touch Control: Simple tap to play or pause music.\n",
    "        <Answer>\n",
    "        Nova Earbuds\"\"\"\n",
    "        f\"<Context>\\n{ctx}\\n<Sentence>\\n{sent}\\n<Question>What is the subject of this sentence given a context?\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "# ------------------ 7. LLM Inference -----------------------------\n",
    "def llm_batch(sent_list: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    prompts, ctx_list = [], []\n",
    "    for s in sent_list:\n",
    "        i   = sent2idx.get(s, -1)\n",
    "        ctx = \" \".join(conversation_list[max(0, i-3):i]) if i >= 0 else \"\"\n",
    "        ctx_list.append(ctx)\n",
    "        prompts.append(_prompt(ctx, s))\n",
    "\n",
    "    toks = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **toks,\n",
    "            max_new_tokens=MAX_NEW,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    dec = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "    return [d.split(\"Answer:\")[-1].strip() for d in dec], ctx_list\n",
    "\n",
    "# ------------------ 9. Batch Classification Logic ------------------------\n",
    "def classify_batch(texts: List[str]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Apply regex first → fallback to LLM if unknown\"\"\"\n",
    "    results: List[Tuple[str, str]] = []\n",
    "    llm_raw: List[str]             = [\"\"] * len(texts)\n",
    "    llm_ctx: List[str]             = [\"\"] * len(texts)\n",
    "\n",
    "    pending_texts, idx_map = [], []\n",
    "\n",
    "    # 1) Regex-based classification\n",
    "    for i, t in enumerate(texts):\n",
    "        lab = clf.classify(t)\n",
    "        if lab[0] == \"Unknown\":\n",
    "            pending_texts.append(t); idx_map.append(i)\n",
    "            results.append(lab)\n",
    "        else:\n",
    "            results.append(lab)\n",
    "\n",
    "    # 2) Call LLM only for unknowns\n",
    "    if pending_texts:\n",
    "        llm_ans, ctx_list = llm_batch(pending_texts)\n",
    "        for ii, ans, ctx in zip(idx_map, llm_ans, ctx_list):\n",
    "            llm_raw[ii] = ans\n",
    "            llm_ctx[ii] = ctx\n",
    "            parsed = clf.classify(f\"{texts[ii]} (about {ans})\", used_llm=True)\n",
    "            results[ii] = parsed if parsed[0] != \"Unknown\" else (\"Unknown\", f\"about {ans}\")\n",
    "\n",
    "    return results, llm_raw, llm_ctx\n",
    "\n",
    "# ------------------ 10. Run Classification ----------------------------\n",
    "from tqdm.notebook import tqdm  # Make sure this is imported at the top\n",
    "\n",
    "def run(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    sents = df[\"Conversation Stream\"].tolist()\n",
    "    preds, llm_texts, ctx_list = [], [], []\n",
    "\n",
    "    print(f\"🚀 Total sentences to process: {len(sents):,}\")\n",
    "    for i in tqdm(range(0, len(sents), BATCH_SIZE), desc=\"🧠 Processing\", unit=\"batch\"):\n",
    "        p_batch, a_batch, c_batch = classify_batch(sents[i : i + BATCH_SIZE])\n",
    "        preds.extend(p_batch)\n",
    "        llm_texts.extend(a_batch)\n",
    "        ctx_list.extend(c_batch)\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"Predicted_Brand\"], out[\"Predicted_Aspect\"] = zip(*preds)\n",
    "    out[\"LLM Raw Answer\"] = llm_texts\n",
    "    out[\"LLM Context\"]    = ctx_list\n",
    "    return out\n",
    "\n",
    "# ------------------ 11. Save Results --------------------------------\n",
    "from zoneinfo import ZoneInfo  # Python 3.9+\n",
    "t0 = time.time()\n",
    "result_df = run(test_df)\n",
    "print(f\"🏁 Completed in {time.time()-t0:.1f}s  ({len(result_df):,} rows)\")\n",
    "\n",
    "out_path = f\"{OUTPUT_NAME}\"\n",
    "result_df.to_excel(out_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNIHBnETDZyBurSQdVMW6m0",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2c750598fbdc4a8a89f381c3f0d0863e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f0dac876bfa4d4faac6cb0a8e8a351f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_82bc4e7d31d44dd4bba5e0cf3bb317dc",
       "IPY_MODEL_6a8d3d180b4b4733b5d3f4b0a92064fe",
       "IPY_MODEL_3ef3c323087c415cb0f5df370ff43085"
      ],
      "layout": "IPY_MODEL_b16a60258e9545a8a1e5a2da39216346"
     }
    },
    "3ef3c323087c415cb0f5df370ff43085": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb8cdb32282940ee8cc3073756b1c91a",
      "placeholder": "​",
      "style": "IPY_MODEL_521f20ef26d44c1da748dc554e112876",
      "value": " 696/696 [1:34:52&lt;00:00,  6.94s/batch]"
     }
    },
    "521f20ef26d44c1da748dc554e112876": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "53fe259db6d6453dad2aa49238d5cf5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a8d3d180b4b4733b5d3f4b0a92064fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf4fb780a01b47409a6acf0f1465906a",
      "max": 696,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_53fe259db6d6453dad2aa49238d5cf5f",
      "value": 696
     }
    },
    "82bc4e7d31d44dd4bba5e0cf3bb317dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c750598fbdc4a8a89f381c3f0d0863e",
      "placeholder": "​",
      "style": "IPY_MODEL_a0aca995aa584bdbbf88f1ac272df504",
      "value": "🧠 Processing: 100%"
     }
    },
    "a0aca995aa584bdbbf88f1ac272df504": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b16a60258e9545a8a1e5a2da39216346": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf4fb780a01b47409a6acf0f1465906a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb8cdb32282940ee8cc3073756b1c91a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
