from google.colab import drive
MODEL_CHECKPOINT_PATH = ""
INPUT_PATH_1 = "//.xlsx"
INPUT_PATH_2 = "//.xlsx"
OUTPUT_PATH = "//.xlsx"




# ------------------ 1. Modules & Environment Setup ----------------------
import re, yaml, time
from pathlib import Path
from typing import Dict, List, Tuple

import pandas as pd
import torch
from transformers import AutoTokenizer
from auto_gptq import AutoGPTQForCausalLM

INPUT_CSV      = f"{INPUT_PATH_1}"
CONTEXT_XLSX   = f"{INPUT_PATH_2}"
CONTEXT_SHEET  = ""
BATCH_SIZE     = 32
OUTPUT_NAME    = f"{OUTPUT_PATH}"
BRAND_YML = Path(".yaml")
ASPT_YML  = Path(".yaml")

MODEL_ID      = "TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ"  # 4-bit quantized model
MAX_INPUT     = 8192  # Llama 3 8B context window
MAX_NEW       = 32
DEVICE        = "cuda" if torch.cuda.is_available() else "cpu"

# ------------------ 2. Load GPTQ Model ------------------------
print("üîß Loading GPTQ model ‚Ä¶")
model = AutoGPTQForCausalLM.from_quantized(
    MODEL_ID,
    device_map="auto",
    use_safetensors=True,
    trust_remote_code=True,
    inject_fused_attention=True,
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token
model = torch.compile(model)
model.eval()
print("‚úÖ Model ready.")

# ------------------ 3. Load Data ---------------------------
print("üìÑ Loading context ‚Ä¶")
ctx_df = pd.read_excel(CONTEXT_XLSX)
conversation_list = ctx_df["Conversation Stream"].tolist()
sent2idx = {s: i for i, s in enumerate(conversation_list)}
print(f"‚úÖ Context rows: {len(conversation_list):,}")

print("üìÑ Loading test csv ‚Ä¶")
test_df = pd.read_excel(INPUT_CSV)
print(f"‚úÖ Test rows   : {len(test_df):,}")

# =============================================================
# 4. YAML ‚Üí Regex ‚Üí BrandClassifier  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _load_yaml(path: Path) -> Dict[str, str]:
    with path.open(encoding="utf-8") as f:
        return yaml.safe_load(f)

def _compile_dict(src: Dict[str, str], flags: int = 0) -> Dict[str, re.Pattern]:
    return {k: re.compile(v, flags) for k, v in src.items()}

class BrandClassifier:
    """Regex-based classifier with LLM fallback"""

    APPLE_REQ_ASP = {
        'Camera General', 'Design General', 'Chipset General',
        'Sustainability General', 'Portrait Studio',
        'On-device', 'Grip', "Thin", "AI General"
    }

    def __init__(self) -> None:
        brand_src  = _load_yaml(BRAND_YML)
        aspect_src = _load_yaml(ASPT_YML)
        self.brand_rx  = _compile_dict(brand_src)
        self.aspect_rx = _compile_dict(aspect_src)
        self._apple_kw_rx = re.compile(
            r"(?i)\b(?:apple|apple's|iphone|ÏïÑÏù¥Ìè∞|Ïï†Ìîå)\b"
        )

    def classify(self, text: str, used_llm: bool = False) -> Tuple[str, str]:
        # 1) Try brand first
        for brand, brx in self.brand_rx.items():
            if brx.search(text):
                if brand == "Apple":
                    for asp, arx in self.aspect_rx.items():
                        if arx.search(text):
                            return "Apple", asp
                return brand, brand

        # 2) No brand ‚Üí check aspect
        for asp, arx in self.aspect_rx.items():
            if arx.search(text):
                if asp not in self.APPLE_REQ_ASP:
                    return "Apple", asp
                if self._apple_kw_rx.search(text):
                    return "Apple", asp
                return "Unknown", "brand_required_aspect_detected"

        # 3) Nothing matched
        return "Unknown", "general"

clf = BrandClassifier()

# ------------------ 6. LLM Prompt Template -------------------------
SYS = "You are an intelligent assistant."
def _prompt(ctx: str, sent: str) -> str:
    return (
        f"{SYS}\nGiven a context, identify what the target sentence is describing. "
        "Answer with one noun phrase only.\n"
        """        [Example 1]
        <Context>
        Flagship phones with great cameras have been IP-rated for a few years now.
        I have had an Apple iPhone 15 Pro with me for a while now, but I never truly checked how effective the IP rating is except for a few splashes and washes here and there.
        It gave me more confidence to take it along for walks even if it drizzled or light rains and have done so without much thought.
        <Sentence>
        It gave me more confidence to take it along for walks even if it drizzled or light rains and have done so without much thought.
        <Answer>
        IP rating of iPhone 15 Pro

        [Example 2]
        <Context>
        Nova Earbuds come with noise cancellation and customizable touch controls.
        - Intuitive Touch Control: Simple tap to play or pause music.
        <Sentence>
        - Intuitive Touch Control: Simple tap to play or pause music.
        <Answer>
        Nova Earbuds"""
        f"<Context>\n{ctx}\n<Sentence>\n{sent}\n<Question>What is the subject of this sentence given a context?\nAnswer:"
    )

# ------------------ 7. LLM Inference -----------------------------
def llm_batch(sent_list: List[str]) -> Tuple[List[str], List[str]]:
    prompts, ctx_list = [], []
    for s in sent_list:
        i   = sent2idx.get(s, -1)
        ctx = " ".join(conversation_list[max(0, i-3):i]) if i >= 0 else ""
        ctx_list.append(ctx)
        prompts.append(_prompt(ctx, s))

    toks = tokenizer(
        prompts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=MAX_INPUT,
    ).to(DEVICE)

    with torch.no_grad():
        out = model.generate(
            **toks,
            max_new_tokens=MAX_NEW,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=False,
        )
    dec = tokenizer.batch_decode(out, skip_special_tokens=True)
    return [d.split("Answer:")[-1].strip() for d in dec], ctx_list

# ------------------ 9. Batch Classification Logic ------------------------
def classify_batch(texts: List[str]) -> List[Tuple[str, str]]:
    """Apply regex first ‚Üí fallback to LLM if unknown"""
    results: List[Tuple[str, str]] = []
    llm_raw: List[str]             = [""] * len(texts)
    llm_ctx: List[str]             = [""] * len(texts)

    pending_texts, idx_map = [], []

    # 1) Regex-based classification
    for i, t in enumerate(texts):
        lab = clf.classify(t)
        if lab[0] == "Unknown":
            pending_texts.append(t); idx_map.append(i)
            results.append(lab)
        else:
            results.append(lab)

    # 2) Call LLM only for unknowns
    if pending_texts:
        llm_ans, ctx_list = llm_batch(pending_texts)
        for ii, ans, ctx in zip(idx_map, llm_ans, ctx_list):
            llm_raw[ii] = ans
            llm_ctx[ii] = ctx
            parsed = clf.classify(f"{texts[ii]} (about {ans})", used_llm=True)
            results[ii] = parsed if parsed[0] != "Unknown" else ("Unknown", f"about {ans}")

    return results, llm_raw, llm_ctx

# ------------------ 10. Run Classification ----------------------------
from tqdm.notebook import tqdm  # Make sure this is imported at the top

def run(df: pd.DataFrame) -> pd.DataFrame:
    sents = df["Conversation Stream"].tolist()
    preds, llm_texts, ctx_list = [], [], []

    print(f"üöÄ Total sentences to process: {len(sents):,}")
    for i in tqdm(range(0, len(sents), BATCH_SIZE), desc="üß† Processing", unit="batch"):
        p_batch, a_batch, c_batch = classify_batch(sents[i : i + BATCH_SIZE])
        preds.extend(p_batch)
        llm_texts.extend(a_batch)
        ctx_list.extend(c_batch)

    out = df.copy()
    out["Predicted_Brand"], out["Predicted_Aspect"] = zip(*preds)
    out["LLM Raw Answer"] = llm_texts
    out["LLM Context"]    = ctx_list
    return out

# ------------------ 11. Save Results --------------------------------
from zoneinfo import ZoneInfo  # Python 3.9+
t0 = time.time()
result_df = run(test_df)
print(f"üèÅ Completed in {time.time()-t0:.1f}s  ({len(result_df):,} rows)")

out_path = f"{OUTPUT_NAME}"
result_df.to_excel(out_path, index=False)
