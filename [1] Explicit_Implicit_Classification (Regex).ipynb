{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fH0XB7ThHdVU"
   },
   "source": [
    "# 1. Logic for detecting multiple aspects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1751875446329,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "xctiTwTpqB1w"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import re\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1. Configuration\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "BRAND_YML = Path(\n",
    "    \"\"\n",
    ")\n",
    "ASPT_YML = Path(\n",
    "    \\"\n",
    ")\n",
    "\n",
    "# Aspects that are only meaningful for Apple devices\n",
    "APPLE_ONLY_ASPECTS = {\n",
    "    \"Camera General\", \"Design General\", \"Chipset General\",\n",
    "    \"Sustainability General\", \"Portrait Studio\",\n",
    "    \"On-device\", \"Grip\", \"Thin\", \"AI General\",\n",
    "}\n",
    "\n",
    "APPLE_KW_RX = re.compile(r\"(?i)\\b(?:apple|apple's|iphone|ÏïÑÏù¥Ìè∞|Ïï†Ìîå)\\b\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2. Helper Functions\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def _load_yaml(path: Path) -> Dict[str, str]:\n",
    "    \"\"\"Load a YAML file as a dictionary\"\"\"\n",
    "    with path.open(encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def _compile_dict(src: Dict[str, str], flags: int = 0) -> Dict[str, re.Pattern]:\n",
    "    \"\"\"Compile a dictionary of regex patterns\"\"\"\n",
    "    return {k: re.compile(v, flags) for k, v in src.items()}\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 3. Main Class\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class BrandClassifier:\n",
    "    def __init__(self) -> None:\n",
    "        self.brand_rx  = _compile_dict(_load_yaml(BRAND_YML), flags=re.I)\n",
    "        self.aspect_rx = _compile_dict(_load_yaml(ASPT_YML), flags=re.I)\n",
    "\n",
    "    def classify(\n",
    "        self, text: str, used_llm: bool = False\n",
    "    ) -> Tuple[List[str], List[str], List[str], List[str]]:\n",
    "        \"\"\"Extract brand/aspect mentions from text and return them as a 4-tuple\"\"\"\n",
    "\n",
    "        brand_hits   = [(br, m.start()) for br, rx in self.brand_rx.items() if (m := rx.search(text))]\n",
    "        aspect_hits  = [(ap, m.start()) for ap, rx in self.aspect_rx.items()  if (m := rx.search(text))]\n",
    "\n",
    "        brand_hits.sort(key=lambda x: x[1])\n",
    "        aspect_hits.sort(key=lambda x: x[1])\n",
    "\n",
    "        detected_brands   = [b for b, _ in brand_hits]\n",
    "        detected_aspects  = [a for a, _ in aspect_hits]\n",
    "\n",
    "        # 2) Handle Apple-only aspects\n",
    "        final_brands, final_aspects = [], []\n",
    "        for asp in detected_aspects:\n",
    "            if asp not in APPLE_ONLY_ASPECTS:\n",
    "                # General aspects ‚Üí assume Apple if no brand specified\n",
    "                final_brands.append(\"Apple\")\n",
    "                final_aspects.append(asp)\n",
    "            else:\n",
    "                # For Apple-specific aspects, confirm actual Apple mention\n",
    "                if \"Apple\" in detected_brands or APPLE_KW_RX.search(text):\n",
    "                    final_brands.append(\"Apple\")\n",
    "                    final_aspects.append(asp)\n",
    "                else:\n",
    "                    # Mark invalid aspect to allow for human post-processing\n",
    "                    final_brands.append(\"Unknown\")\n",
    "                    final_aspects.append(\"brand_required_aspect_detected\")\n",
    "\n",
    "        # 3) If only brand detected but no aspect, mirror brands to aspects\n",
    "        if not detected_aspects and detected_brands:\n",
    "            final_brands  = list(set(detected_brands))\n",
    "            final_aspects = final_brands.copy()\n",
    "\n",
    "        # 4) Fallback defaults\n",
    "        if not final_brands:\n",
    "            final_brands = [\"Unknown\"]\n",
    "        if not final_aspects:\n",
    "            final_aspects = [\"general\"]\n",
    "\n",
    "        # 5) Add other brands also as aspects\n",
    "        others = [b for b in detected_brands if b != \"Apple\"]\n",
    "        final_brands.extend(others)\n",
    "        final_aspects.extend(others)\n",
    "\n",
    "        # 6) Clean up: remove duplicates and invalid flags\n",
    "        final_brands  = [b for b in dict.fromkeys(final_brands)  if b != \"Unknown\"]\n",
    "        final_aspects = [\n",
    "            a for a in dict.fromkeys(final_aspects)\n",
    "            if a not in {\"Unknown\", \"brand_required_aspect_detected\"}\n",
    "        ]\n",
    "\n",
    "        return final_brands, final_aspects, detected_brands, detected_aspects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jA0HQmA0gVHl"
   },
   "source": [
    "## Sample Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1751875451241,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "c4s97PEgqM5o",
    "outputId": "4dc5c0a2-60fc-4eb4-d809-ae82a9c52001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Î∏åÎûúÎìú Î™©Î°ù: []\n",
      "üß© Ïñ¥Ïä§ÌéôÌä∏ Î™©Î°ù: []\n",
      "- Raw Î∏åÎûúÎìú : []\n",
      "- Raw Ïñ¥Ïä§ÌéôÌä∏ : ['Design General']\n"
     ]
    }
   ],
   "source": [
    "classifier = BrandClassifier()\n",
    "\n",
    "# txt = (\"If you want to find out about the camera, processor and ‚Äì especially ‚Äì AI improvements in those devices, read our hands-on Samsung Galaxy S25 review, hands-on Samsung Galaxy S25 Plus review and our hands-on Samsung Galaxy S25 Ultra review.\")\n",
    "txt = \"\"\"From the city streets to the open road, Changan‚Äôs electric vehicles have managed to combine sleek design, rugged capability, and sophisticated technology.\"\"\"\n",
    "\n",
    "brands, aspects, detected_b, detected_a = classifier.classify(txt)\n",
    "\n",
    "print(\"üì¶ Brand list:\", brands)        # ['Apple', 'samsung']\n",
    "print(\"üß© Aspect list:\", aspects)      # ['Thin', 'AI General']\n",
    "print(\"- Raw brands:\", detected_b)    # ['Apple']\n",
    "print(\"- Raw aspects:\", detected_a)   # ['Thin', 'AI General']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Pt7-yjytF2L"
   },
   "source": [
    "# 3. [Package code] Split rows with multiple aspects into separate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56909,
     "status": "ok",
     "timestamp": 1751875508988,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "VG1rX4thdRuK",
    "outputId": "fb051d49-6c88-4a1a-b3df-9cfc0dc2caaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "\n",
    "MODE = \"prod\"\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "INPUT_PATH = Path(\"/content/translated_crawl_results.xlsx\")\n",
    "df = pd.read_excel(INPUT_PATH)  # Skip this if using external main module\n",
    "\n",
    "if MODE == \"dev\":\n",
    "    # For dev mode: define the true aspect and remove duplicates\n",
    "    df[\"True_Aspect\"] = df[\"Aspect\"].fillna(df[\"Keywords(Company)\"])\n",
    "    df.drop_duplicates(['Media Title', 'Conversation Stream', 'True_Aspect'], inplace=True)\n",
    "\n",
    "clf = BrandClassifier()\n",
    "rows = []\n",
    "\n",
    "for conv_id, g in df.groupby(\"Conversation Stream\", sort=False):\n",
    "    pred_brands, pred_aspects, *_ = clf.classify(str(conv_id))\n",
    "    url = g[\"url\"].iloc[0] if \"url\" in g.columns else np.nan\n",
    "    pred_map = {asp.casefold(): br for br, asp in zip(pred_brands, pred_aspects)}\n",
    "\n",
    "    if MODE == \"dev\":\n",
    "        for true_val, senti in zip(g[\"True_Aspect\"], g[\"Sentiment\"]):\n",
    "            norm_true = str(true_val).strip().casefold()\n",
    "            if norm_true in pred_map:\n",
    "                rows.append((g[\"Media Title\"].iloc[0], conv_id, true_val, norm_true, pred_map[norm_true], senti, url))\n",
    "                del pred_map[norm_true]\n",
    "            else:\n",
    "                rows.append((g[\"Media Title\"].iloc[0], conv_id, true_val, np.nan, np.nan, senti, url))\n",
    "\n",
    "    if MODE == \"prod\" and not pred_map:\n",
    "        # If no prediction made in prod mode, log minimal entry\n",
    "        rows.append((g[\"Media Title\"].iloc[0], conv_id, np.nan, np.nan, url))\n",
    "\n",
    "    for asp_cf, br in pred_map.items():\n",
    "        asp = next(a for a in pred_aspects if a.casefold() == asp_cf)\n",
    "        if MODE == \"dev\":\n",
    "            rows.append((g[\"Media Title\"].iloc[0], conv_id, np.nan, asp, br, \"NEED_LABELING\", url))\n",
    "        else:\n",
    "            rows.append((g[\"Media Title\"].iloc[0], conv_id, asp, br, url))\n",
    "\n",
    "# Create DataFrame depending on the mode\n",
    "if MODE == \"dev\":\n",
    "    df_expanded = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"Media Title\",\n",
    "            \"Conversation Stream\",\n",
    "            \"True_Aspect\",\n",
    "            \"Predicted_Aspect\",\n",
    "            \"Predicted_Brand\",\n",
    "            \"True_Sentiment\",\n",
    "            \"url\"\n",
    "        ],\n",
    "    )\n",
    "else:\n",
    "    df_expanded = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"Media Title\",\n",
    "            \"Conversation Stream\",\n",
    "            \"Predicted_Aspect\",\n",
    "            \"Predicted_Brand\",\n",
    "            \"url\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "df_implicit = df_expanded.copy()\n",
    "\n",
    "# Explicit determination: Aspect is not \"general\"\n",
    "df_explicit = df_expanded[\n",
    "    df_expanded[\"Predicted_Aspect\"].notna() &\n",
    "    (df_expanded[\"Predicted_Aspect\"].str.lower() != \"general\")\n",
    "].copy()\n",
    "\n",
    "# File output paths\n",
    "OUTPUT_PATH_1 = \"/content/implicit_case.xlsx\"\n",
    "OUTPUT_PATH_2 = \"/content/Aspect_Extraction_explicit.xlsx\"\n",
    "\n",
    "df_implicit.to_excel(OUTPUT_PATH_1, index=False, engine=\"openpyxl\")\n",
    "df_explicit.to_excel(OUTPUT_PATH_2, index=False, engine=\"openpyxl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D23hgVQ8Ejba"
   },
   "source": [
    "## Preprocessing for human errors (only in dev mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "error",
     "timestamp": 1750902325739,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "nvajDOPR4akq",
    "outputId": "6442b2dc-9cee-456c-8463-e2fa7ce8ac37"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_expanded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-24-384331964.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mMODE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"dev\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mdf_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_expanded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mdf_prod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_expanded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_expanded' is not defined"
     ]
    }
   ],
   "source": [
    "if MODE == \"dev\":\n",
    "  df_dev = df_expanded.copy()\n",
    "else:\n",
    "  df_prod = df_expanded.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8W4QmSV_aQkY"
   },
   "outputs": [],
   "source": [
    "if MODE == \"dev\":\n",
    "    df_dev_processed = df_dev.copy()\n",
    "    df_dev_processed[\"Manual_Labeling\"] = False\n",
    "    df_dev_processed[\"Implicit\"] = False\n",
    "\n",
    "    drop_list   = []          # To collect rows to drop later\n",
    "    update_rows = {}          # {row indices: new predicted values}\n",
    "\n",
    "    for cid, grp in df_dev_processed.groupby(\"Conversation Stream\"):\n",
    "\n",
    "        # 1) Single row & manual labeling\n",
    "        if len(grp) == 1:\n",
    "            i = grp.index[0]\n",
    "\n",
    "            # a) Manual labeling if True_Aspect is missing but Predicted_Aspect exists\n",
    "            if pd.isna(grp.at[i, \"True_Aspect\"]) and pd.notna(grp.at[i, \"Predicted_Aspect\"]):\n",
    "                df_dev_processed.at[i, \"Manual_Labeling\"] = True\n",
    "\n",
    "            # b) Implicit flag if Predicted_Aspect is missing\n",
    "            if pd.isna(grp.at[i, \"Predicted_Aspect\"]):\n",
    "                df_dev_processed.at[i, \"Implicit\"] = True\n",
    "            continue  # Skip the rest for single-row groups\n",
    "\n",
    "        # 2) Check if any row has True_Aspect == \"apple\"\n",
    "        mask_a = grp[\"True_Aspect\"].astype(str).str.strip().str.casefold() == \"apple\"\n",
    "        apple_idx = grp.index[mask_a]\n",
    "\n",
    "        if len(apple_idx):\n",
    "            # a) Schedule those for deletion\n",
    "            drop_list.extend(apple_idx)\n",
    "\n",
    "            # b) Select rows to fill True_Aspect where it's NaN and Predicted_Aspect is available\n",
    "            cond_fill = grp.index.difference(apple_idx)  # (1) Exclude \"apple\" rows\n",
    "            cond_fill = cond_fill.intersection(          # (2) Rows where:\n",
    "                grp.index[grp[\"True_Aspect\"].isna() & grp[\"Predicted_Aspect\"].notna()]\n",
    "            )\n",
    "\n",
    "            # (3) Schedule updates: {row index : new True_Aspect (= Predicted_Aspect)}\n",
    "            update_true = {i: df_dev_processed.at[i, \"Predicted_Aspect\"] for i in cond_fill}\n",
    "\n",
    "            # (4) Merge into update_rows dictionary\n",
    "            update_rows.update(update_true)\n",
    "\n",
    "            # ‚ñ∂ new: Mark the group as \"if_has_apple\"\n",
    "            df_dev_processed.loc[grp.index, \"Apple_branch\"] = \"if_has_apple\"\n",
    "\n",
    "        # 3) No \"apple\" in True_Aspect, and no predicted aspect is \"apple\"\n",
    "        else:\n",
    "            has_pa_apple = (\n",
    "                grp[\"Predicted_Aspect\"]\n",
    "                .astype(str).str.strip().str.casefold()\n",
    "                .eq(\"apple\")\n",
    "            ).any()\n",
    "\n",
    "            if not has_pa_apple:\n",
    "                # (1) Select rows where Predicted_Aspect is not NaN\n",
    "                mask_notna = df_dev_processed.loc[grp.index, \"Predicted_Aspect\"].notna()\n",
    "                target_idx = grp.index[mask_notna]\n",
    "\n",
    "                # Fill True_Aspect with the corresponding Predicted_Aspect\n",
    "                df_dev_processed.loc[target_idx, \"True_Aspect\"] = (\n",
    "                    df_dev_processed.loc[target_idx, \"Predicted_Aspect\"]\n",
    "                )\n",
    "\n",
    "            # ‚ñ∂ new: Mark the group as \"else_no_apple\"\n",
    "            df_dev_processed.loc[grp.index, \"Apple_branch\"] = \"else_no_apple\"\n",
    "\n",
    "    # ‚îÄ‚îÄ Apply scheduled drops and updates ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    if drop_list:\n",
    "        df_dev_processed.drop(index=drop_list, inplace=True)\n",
    "\n",
    "    for j, new_val in update_rows.items():\n",
    "        if j in df_dev_processed.index:  # Safety check\n",
    "            df_dev_processed.at[j, \"True_Aspect\"] = new_val\n",
    "\n",
    "    # ‚îÄ‚îÄ (dev) Split into explicit & implicit cases ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    df_dev_explicit = df_dev_processed[df_dev_processed['Implicit'] == False]\n",
    "    df_implicit = df_dev_processed[df_dev_processed['Implicit'] == True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vagVMLUqt5ge"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def merge_rows(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Receives a group `g` (same Conversation Stream).\n",
    "    If conditions are met, merges 2 rows into 1 row;\n",
    "    otherwise, returns the group unchanged.\n",
    "    \"\"\"\n",
    "    if len(g) == 2:\n",
    "        # Check if there is exactly one non-NaN value per field\n",
    "        true_vals    = g[\"True_Aspect\"].dropna().unique()\n",
    "        true_sent    = g[\"True_Sentiment\"].dropna().unique()\n",
    "        pred_vals    = g[\"Predicted_Aspect\"].dropna().unique()\n",
    "        pred_brands  = g[\"Predicted_Brand\"].dropna().unique()\n",
    "\n",
    "        if len(true_vals) == 1 and len(pred_vals) == 1:\n",
    "            # Create a new row by copying the first row and updating values\n",
    "            new_row = g.iloc[0].copy()\n",
    "            new_row[\"True_Aspect\"]       = true_vals[0]\n",
    "            new_row[\"Predicted_Aspect\"]  = pred_vals[0]\n",
    "            new_row[\"Predicted_Brand\"]   = pred_brands[0]\n",
    "            new_row[\"True_Sentiment\"]    = true_sent[0]\n",
    "            return pd.DataFrame([new_row])  # Return single-row DataFrame\n",
    "\n",
    "    # If conditions are not satisfied, return the original group\n",
    "    return g\n",
    "\n",
    "if MODE == \"dev\":\n",
    "    df_explicit = (\n",
    "        df_dev_explicit\n",
    "            .groupby(\"Conversation Stream\", group_keys=False)\n",
    "            .apply(merge_rows)\n",
    "            .reset_index(drop=True)  # Optional: reset index\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 70,
     "status": "error",
     "timestamp": 1751601601734,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "_XXbBW3nxwI4",
    "outputId": "e3841b01-1433-4779-fd9a-b512104595d5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_prod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-6-701549432.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mMODE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"prod\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mdf_implicit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_prod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_prod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_Aspect'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mdf_explicit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_prod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_prod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_Aspect'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_prod' is not defined"
     ]
    }
   ],
   "source": [
    "if MODE == \"prod\":\n",
    "  df_implicit = df_prod[df_prod['Predicted_Aspect'].isna()]\n",
    "  df_explicit = df_prod[~(df_prod['Predicted_Aspect'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "error",
     "timestamp": 1751601609620,
     "user": {
      "displayName": "KEARNEY SEMX",
      "userId": "06834010073227309486"
     },
     "user_tz": -540
    },
    "id": "V4eGRMtNTBRh",
    "outputId": "0970bf24-4e29-4b66-e34d-254374e61b1e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_implicit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-7-2045134337.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_implicit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{OUTPUT_PATH_1}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_explicit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{OUTPUT_PATH_2}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_implicit' is not defined"
     ]
    }
   ],
   "source": [
    "df_implicit.to_excel(f\"{OUTPUT_PATH_1}\", index=False)\n",
    "df_explicit.to_excel(f\"{OUTPUT_PATH_2}\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOOl8KHOLqNmYN4kV4yNWx9",
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "1W7eH9TgIpCFx80dxV26oMLEqOQkGg2rr",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
